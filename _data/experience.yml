# Jobs

- company: Ahrefs Singapore
  position: Software Engineer
  duration: July 2023 &mdash;
  summary:
    Ahrefs is one of the most popular software for SEO and content marketing. AhrefsBot is the most active SEO crawler on the internet, visiting over 8 billion web pages every 24 hours and updating its index every 15 &ndash; 30 minutes. All crawled and derived data are stored in ClickHouse, which is the backbone of Ahrefs infrastructure. I'm the foundation engineer of ClickHouse team at Ahrefs, focusing 100% my time on building data infrastructure. I'm responsible for developing and operating ClickHouse on > 1000 machines, holding > 100PB / 2EiB of compressed / uncompressed data, powering all core services in Ahrefs. At the same time, I've been contributing back to ClickHouse, with PRs in almost all major releases since 2022. Some of my notable works&colon;<p />
    - Key value interface&colon; serving millions of key-value queries from ClickHouse, enable business to build more exciting features (e.g sorting websites by various metrics) that cannot be done if using plain SQL because the server will be overloaded. The new key-value interface uses self-developed, parallelized TCP server instead of traditional Poco library, and completely bypass the overhead of parsing SQL.<p />
    - Geo-replication&colon; enable ClickHouse to replicate data across data centers in different continents with limited bandwidth (upstream PR <a href="https://github.com/ClickHouse/ClickHouse/pull/52856">#52856</a>). This help us to set up multiple replicas in different regions to serve our customers with low latency and high availability.<p />
    - Vertical FINAL&colon; a cache-friendly implementation of primary key deduplication, improve P99 latency by more than 50% in Ahrefs cluster (upstream PR <a href="https://github.com/ClickHouse/ClickHouse/pull/54366">#54366</a>).<p />
    - Disk management and evacuation&colon; automatically include / exclude disks and backfill data if needed when new drives are added / removed / broken and allow evacuate ClickHouse table data from one disk to another disk without downtime (not fully upstreamed, related PRs <a href="https://github.com/ClickHouse/ClickHouse/pull/56367">#56367</a> and <a href="https://github.com/ClickHouse/ClickHouse/pull/58285">#58285</a>). This feature helps Ahrefs to operate a relatively large ClickHouse cluster with a one-person team.<p />
    - Column level compression block sizes &colon; different compression block size for different columns in the same table, allow building table with high compression ratio and reducing memory in SELECT queries (upstream PR <a href="https://github.com/ClickHouse/ClickHouse/pull/55201">#55201</a>).<p />
    - Rocksdb bulk loading&colon; improve the performance of ClickHouse when loading data from RocksDB (upstream PR <a href="https://github.com/ClickHouse/ClickHouse/pull/59163">#59163</a>). Together with other optimizations, our pipeline can finish populating dictionaries in less than half a day, originally it took more than a day.<p />
    - Materialized CTE&colon; enable ClickHouse to materialize common table expressions, improve the performance of complex queries (upstream PR <a href="https://github.com/ClickHouse/ClickHouse/pull/61086">#61086</a>). This feature also helps backend engineers to reduce the complexity of their queries and improve the readability of the code.<p />
    - ClickHouse Keeper migration&colon; develop a solution to migrate replicated tables from one keeper to another via a SQL query with minimal downtime. We are able to migrate more than 40 thousand tables from old Zookeeper cluster to ClickHouse Keeper cluster within a day without affecting any user. Note that traditional method of converting Zookeeper snapshot to ClickHouse Keeper doesn't work for us, because we already have a large number of tables using ClickHouse Keeper. Moving to ClickHouse Keeper helps our cluster more stable and significantly improve replication performance.<p />
    - Limiting historical merges&colon; limit the number of historical merges to prevent ClickHouse from executing too many big merges at the same time, which give no resources to merge new data and reduce query performance (related upstream PR <a href="https://github.com/ClickHouse/ClickHouse/pull/53405">#53045</a>). This feature allow us to have more aggressive merging of historical data without affecting the performance of the cluster and save a lot of disk space. The rough estimation is we save nearly a million USD in SSD cost in 2024 since we implemented this feature.<p />
    - Currently, I'm leading the effort to move ClickHouse to object storage (we use Ceph) for better scalability and cost efficiency (related upstream PR <a href="https://github.com/ClickHouse/ClickHouse/pull/65740">#65740</a> support ceph rados object storage). <p />
    And many more bug fixes and general performance optimization that help ClickHouse serving hundred thousands queries per minute while being stable and reliable. The customer-facing p99 query latency is close to 1 seconds and the downtime rate is minimal even during upgrading or network unstable. We don't officially record the cluster availability yet, but the last time we have a non-minor downtime is when <a href="https://www.forbes.com/sites/zacharyfolk/2024/03/04/four-fiber-optic-cables-damaged-in-red-sea-heres-what-we-know/">four optic cables were cut in RedSea</a>. <p />
    Beside ClickHouse, I also spend time to explore various exciting opensource projects (RocksDB, Ceph, MySQL...). In the long term, our vision is to build a data platform that can scale, easy to use and operate, and enable the business to create more exciting features in Ahrefs products. We also thrilled to contribute back our work to the community.<p />

- company: ByteDance Singapore
  position: Software Engineer
  duration: Mar, 2021 &mdash; Jun 2023
  summary: I'm with the Data Platform team to build the next generation analytics DBMS which is used in all ByteDance's products. Our DBMS hosts EiB of data, powering real time and offline analytics, BI, AB testing, and recommendation... across nearly all departments in ByteDance. In Mar 2023, I was promoted to Senior Software Engineer for my contributions to the project. Part of our work was opensourced as ByConity project.

- company: Korea Advanced Institute of Science and Technology
  position: Research Scientist
  duration: June, 2020 &mdash; Dec 2020
  summary: I worked on developing a deep learning serving platform on heterogeneous hardware.

- company: Korea Advanced Institute of Science and Technology
  position: Research Assistant
  duration:  Feb, 2018 &mdash; Feb, 2020
  summary: I did my research at Network and Computing Lab, Department of Electrical Engineering. 


- company: Viettel Corporation
  position: Algorithm Engineer
  duration:   July, 2016  &mdash; Feb, 2018
  summary: I was an intern from July 2016 to August 2017 and became an employee from Sept 2017. I worked at the Viettel Network Technology Center (now Viettel R&D Center). 
